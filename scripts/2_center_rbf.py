# -*- coding: utf-8 -*-
"""2_center_rbf.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W6o7efvOat3ZOXh0Ad9_EpggnJxESIG4
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Lambda, Concatenate, BatchNormalization, Flatten
from tensorflow.keras.constraints import MinMaxNorm
from sklearn.preprocessing import MinMaxScaler

data = pd.read_csv('/content/ag-1.csv')
X = data.iloc[:, :-1].values  # 28 columns of configuration
y = data.iloc[:, -1].values  # 1 column of energy

centers = np.array([[0, 0], [0, 2.89135]])
sigma = 1

def rbf_layer(x, centers, sigma):
    x_expanded = tf.expand_dims(x, axis=-1)
    distances = tf.sqrt(tf.reduce_sum(tf.square(x_expanded - centers), axis=1))
    # Apply the Gaussian RBF function
    rbf_outputs = tf.exp(-tf.square(distances) / (2 * sigma * sigma))
    return rbf_outputs

# Define the model
inputs = Input(shape=(28,))
rbf_outputs_1 = Lambda(lambda x: rbf_layer(x, centers[0], sigma))(inputs)
rbf_outputs_2 = Lambda(lambda x: rbf_layer(x, centers[1], sigma))(inputs)
rbf_outputs = Concatenate()([rbf_outputs_1, rbf_outputs_2])
weight_constraint = MinMaxNorm(min_value=-1.0, max_value=1.0)


# Add a Dense layer after the RBF layer
dense_output = BatchNormalization()(rbf_outputs)
dense_output = Dense(64, activation='sigmoid', use_bias=False, kernel_constraint=weight_constraint)(dense_output)
dense_output = Dense(32, activation='sigmoid', use_bias=False, kernel_constraint=weight_constraint)(dense_output)
dense_output = Dense(16, activation='sigmoid', use_bias=False, kernel_constraint=weight_constraint)(dense_output)
output = Dense(1, use_bias=False, kernel_constraint=weight_constraint)(dense_output)

# Final model
model = Model(inputs=inputs, outputs=output)

# Compile the model
model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01), loss='mse')

class WeightRBFLoggingCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        print(f"Epoch {epoch + 1}:")
        for layer in self.model.layers:
            if hasattr(layer, 'get_weights'):
                weights = layer.get_weights()
                for i, w in enumerate(weights):
                    print(f"Layer {layer.name} - Weight {i} shape: {w.shape}\n{w}")
        rbf_1_output = self.model.get_layer(index=1).output
        rbf_2_output = self.model.get_layer(index=2).output
        print(f"RBF Layer 1 Output: {rbf_1_output}")
        print(f"RBF Layer 2 Output: {rbf_2_output}")

class LossThresholdCallback(tf.keras.callbacks.Callback):
    def __init__(self, threshold):
        super(LossThresholdCallback, self).__init__()
        self.threshold = threshold

    def on_epoch_end(self, epoch, logs=None):
        if logs.get('loss') < self.threshold:
            self.model.stop_training = True

loss_threshold_callback = LossThresholdCallback(threshold=0.01)

# Train the model
history = model.fit(X, y, epochs=10000, batch_size=64, callbacks=[loss_threshold_callback, WeightRBFLoggingCallback()])

# Summary of the model
model.summary()

predictions = model.predict(X)

# Print the results
print(f"Actual values: {y[:10]}")
print(f"Predicted values: {predictions[:10].flatten()}")

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 5))
plt.scatter(y,predictions, alpha=0.5)
plt.xlabel('True Energy')
plt.ylabel('Predicted Energy')
plt.title('Scatter Plot of True vs Predicted Energy')
plt.plot([min(y), max(y)], [min(y), max(y)], color='red', linestyle='--')  # Line y=x for reference
plt.ylim(0,1)
plt.show()

