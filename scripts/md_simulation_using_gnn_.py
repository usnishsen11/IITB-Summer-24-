# -*- coding: utf-8 -*-
"""md simulation using gnn .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vPEoNNyieXuzVw47jHeg7mXDHWmk2Mgd
"""

import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Example data generation function
def generate_data(num_samples):
    configurations = []
    energies = []
    for _ in range(num_samples):
        # Random molecular configurations
        configuration = np.random.rand(4)  # Assumin configuration with 4 molecules
        print(configuration)
        energy = np.random.rand(1)  # Associating energy for each configurations
        configurations.append(configuration)
        energies.append(energy)
    return np.array(configurations), np.array(energies)

#data generated
num= 1000
configurations, energies=generate_data(num)

configurations

energies

# Splitting the dataset
configurations_train, configurations_test, energies_train, energies_test = train_test_split(configurations, energies, test_size=0.2, random_state=42)

# Feature scaling
scaler = StandardScaler()
configurations_train_scaled = scaler.fit_transform(configurations_train)
configurations_test_scaled = scaler.transform(configurations_test)

#MLP Model
model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(4,)),  # Input layer with 4 molecules
    tf.keras.layers.Dense(64, activation='relu'),  # Hidden layer with 64 neurons and ReLU activation
    tf.keras.layers.Dense(64, activation='relu'),  # Hidden layer with 64 neurons and ReLU activation
    tf.keras.layers.Dense(1)  # Output layer with single neuron -energy prediction
])

model.compile(optimizer='adam', loss='mse')

#using only 10 epochs for a the small size of generated data
model.fit(configurations_train_scaled, energies_train, epochs=10, batch_size=32, validation_split=0.2)

mse_1 = model.evaluate(configurations_test_scaled, energies_test)
print("Test MSE:", mse_1)

import matplotlib.pyplot as plt

# Predict energy states for the test set
predicted_energies = model.predict(configurations_test_scaled)

# Plot true vs. predicted energy states
plt.figure(figsize=(8, 6))
plt.scatter(energies_test, predicted_energies, color='b', alpha=0.5)
plt.plot([energies_test.min(), energies_test.max()], [energies_test.min(), energies_test.max()], 'k--', lw=2)
plt.xlabel('True Energy')
plt.ylabel('Predicted Energy')
plt.title('True vs. Predicted Energy States')
plt.grid(True)
plt.show()

#model-2 GRU (Gated Recurrent Units)
model_2 = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(10,4)),  # Input layer with 4 nodes(molecules)
    tf.keras.layers.GRU(64, activation='relu', return_sequences=True),
    tf.keras.layers.GRU(64, activation='relu'),  # GRU layer with 64 units and ReLU activation
    tf.keras.layers.Dense(1)# Output layer with single neuron (energy prediction)
  ])

model_2.compile(optimizer='adam', loss='mse')

# Train the model
model_2.fit(configurations_train_scaled, energies_train, epochs=10, batch_size=32, validation_split=0.2)

# Evaluate the model
mse = model.evaluate(configurations_test_scaled, energies_test)
print("Test MSE:", mse)

!pip install spektral
import spektral

pip install --upgrade spektral

#converting data to graphs
def convert_to_graph(configurations):
  graphs = []
  for config in configurations:
    adjc = np.ones((config.shape[0], config.shape[0]))  # Assuming fully connected graph /adjacency matrix
    graph = Graph(x=config, a=adjc)
    graphs.append(graph)
  return graphs

graphs_train = convert_to_graph(configurations_train_scaled)
graphs_test = convert_to_graph(configurations_test_scaled)

configurations

graphs_train

graphs_test

!pip install --upgrade spektral
from spektral.layers import GraphConv

#model -3  Graph Convolution Network
class GCNModel(tf.keras.Model):
    def __init__(self):
        super(GCNModel, self).__init__()
        self.conv1 = GraphConv(64, activation='relu')
        self.conv2 = GraphConv(32, activation='relu')
        self.flatten = tf.keras.layers.Flatten()
        self.dense = tf.keras.layers.Dense(1)

    def call(self, inputs):
        x, a = inputs
        x = self.conv1([x, a])
        x = self.conv2([x, a])
        x = self.flatten(x)
        x = self.dense(x)
        return x

model_3=GCNModel()
model_3.compile(optimizer='adam', loss='mse')

import spektral.layers

#Model-4 GAT
model_4 = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(4,)),  # Input layer with 4 molecules
    GraphAttention(64, activation='relu'),  # GAT layer with 64 units and ReLU activation
    tf.keras.layers.Dropout(0.5),  # Dropout for regularization
    GraphAttention(32, activation='relu'),  # GAT layer with 32 units and ReLU activation
    tf.keras.layers.Dropout(0.5),  # Dropout for regularization
    tf.keras.layers.Dense(1)
])

